{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformación de Datos en Machine Learning con Python**\n",
    "\n",
    "Antes de aplicar algoritmos de Machine Learning, es crucial preparar y transformar los datos para mejorar la calidad y el rendimiento del modelo. La transformación de datos incluye técnicas para convertir datos categóricos a numéricos, manejar valores faltantes, normalizar o estandarizar los datos, entre otras. Estas técnicas aseguran que los modelos de ML reciban datos en un formato que maximice su precisión y eficiencia.\n",
    "\n",
    "En este contenido, usaremos un dataset de **Scikit-Learn** y aplicaremos técnicas clásicas de transformación de datos como:\n",
    "\n",
    "1. **One-Hot Encoding**: Convertir variables categóricas en variables binarias.\n",
    "2. **Manejo de Valores Faltantes**: Rellenar o eliminar valores ausentes.\n",
    "3. **Estandarización y Normalización**: Ajustar las características para que tengan una escala similar.\n",
    "4. **Escalado de Características**: Normalización y estandarización.\n",
    "5. **Transformación Logarítmica**: Transformar características sesgadas.\n",
    "\n",
    "### **Dataset Utilizado: Diabetes de Scikit-Learn**\n",
    "\n",
    "Utilizaremos el dataset de **Diabetes** de **Scikit-Learn**, que es un conjunto de datos conocido para problemas de regresión. Este dataset contiene información sobre pacientes y varias características médicas utilizadas para predecir la progresión de la diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019907 -0.017646  \n",
       "1 -0.039493 -0.068332 -0.092204  \n",
       "2 -0.002592  0.002861 -0.025930  \n",
       "3  0.034309  0.022688 -0.009362  \n",
       "4 -0.002592 -0.031988 -0.046641  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset de diabetes\n",
    "data = load_diabetes()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)  # Variables independientes\n",
    "y = pd.Series(data.target, name='Progression')  # Variable dependiente\n",
    "\n",
    "# Mostrar los primeros registros del dataset\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. One-Hot Encoding (Codificación One-Hot)**\n",
    "\n",
    "La codificación One-Hot convierte variables categóricas en variables binarias. Para demostrar esta técnica, usaremos un ejemplo simulado.\n",
    "\n",
    "**Justificación del método One-Hot Encoding**\n",
    "\n",
    "Cuando entrenas un modelo de machine learning, este modelo trabaja con números y matemáticas. Si dejas una variable categórica (como “color” con valores como “rojo”, “verde” o “azul”) tal cual, el modelo no entendería su significado porque esas palabras no tienen un valor numérico que pueda usar en sus cálculos.\n",
    "\n",
    "Ahora, podrías pensar en convertir esos valores a números (por ejemplo, “rojo” = 1, “verde” = 2, “azul” = 3), pero esto introduciría un problema. El modelo podría asumir que hay una relación o un orden entre esos números, como si “verde” (2) fuese mayor o más importante que “rojo” (1), y “azul” (3) fuese aún mayor. Sin embargo, en realidad, “rojo”, “verde” y “azul” son categorías sin orden inherente.\n",
    "\n",
    "Aquí es donde entra el “one hot encoding”. Con esta técnica, creas una columna separada para cada valor posible de la variable categórica, donde pones un “1” para indicar la presencia de ese valor y “0” para su ausencia. Así, para “color”, tendrías:\n",
    "\n",
    "- Una columna para “rojo” (1 si es rojo, 0 si no lo es)\n",
    "- Una columna para “verde” (1 si es verde, 0 si no lo es)\n",
    "- Una columna para “azul” (1 si es azul, 0 si no lo es)\n",
    "\n",
    "Esto permite que el modelo entienda cada categoría como algo independiente, sin suponer que hay una relación o jerarquía entre ellas. De este modo, puede aprender correctamente a partir de los datos.\n",
    "\n",
    "**Ejemplo: Aplicar One-Hot Encoding a una Variable Categórica Simulada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>Category_A</th>\n",
       "      <th>Category_B</th>\n",
       "      <th>Category_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  Category_A  Category_B  Category_C  \n",
       "0 -0.002592  0.019907 -0.017646         1.0         0.0         0.0  \n",
       "1 -0.039493 -0.068332 -0.092204         0.0         0.0         1.0  \n",
       "2 -0.002592  0.002861 -0.025930         1.0         0.0         0.0  \n",
       "3  0.034309  0.022688 -0.009362         0.0         0.0         1.0  \n",
       "4 -0.002592 -0.031988 -0.046641         0.0         1.0         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Crear una variable categórica simulada\n",
    "X['Category'] = np.random.choice(['A', 'B', 'C'], size=len(X))\n",
    "\n",
    "# Aplicar One-Hot Encoding usando OneHotEncoder de sklearn\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "category_encoded = encoder.fit_transform(X[['Category']])\n",
    "\n",
    "# Convertir a DataFrame y concatenar con los datos originales\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=encoder.get_feature_names_out(['Category']))\n",
    "X = pd.concat([X, category_encoded_df], axis=1).drop(columns=['Category'])\n",
    "\n",
    "# Mostrar los datos transformados\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Manejo de Valores Faltantes (Handling Missing Values)**\n",
    "\n",
    "El manejo de valores faltantes es esencial para evitar que los modelos de ML arrojen errores o produzcan resultados sesgados. Los métodos comunes incluyen la imputación de valores (relleno) y la eliminación de filas o columnas con datos faltantes.\n",
    "\n",
    "**Justificación de manejo de valores faltantes**\n",
    "\n",
    "Los valores faltantes en un dataset pueden ser problemáticos para entrenar modelos de machine learning porque los modelos dependen de los datos completos para aprender correctamente. Aquí te explico por qué:\n",
    "\n",
    "1. **Modelos necesitan datos completos para los cálculos**: Los modelos de machine learning hacen cálculos matemáticos con los datos. Si hay valores faltantes (como \"NaN\" o \"null\"), el modelo no sabe qué hacer con ellos y no puede completar los cálculos necesarios. Por ejemplo, si falta el valor de una característica importante (como la edad o el ingreso), el modelo no puede incluir esa información en su proceso de aprendizaje, lo que puede llevar a errores o resultados inexactos.\n",
    "\n",
    "2. **Valores faltantes pueden distorsionar el entrenamiento**: Si hay muchos valores faltantes, el modelo puede no aprender correctamente la relación entre las diferentes variables del dataset. Esto es como intentar resolver un rompecabezas con piezas perdidas; es mucho más difícil ver la imagen completa.\n",
    "\n",
    "3. **Los valores faltantes pueden introducir sesgos**: Si algunas categorías o grupos de datos tienen más valores faltantes que otros, el modelo puede volverse sesgado. Por ejemplo, si faltan más datos de un grupo específico (como las edades de un cierto rango), el modelo puede aprender patrones incorrectos o ser menos preciso para ese grupo.\n",
    "\n",
    "Para evitar estos problemas, normalmente se deben **eliminar las filas con valores faltantes** o, más comúnmente, **rellenar los valores faltantes** con técnicas como la media, la mediana, o un valor más común, para que el modelo tenga un dataset completo y pueda aprender de manera más efectiva.\n",
    "\n",
    "**Extra: Método de indicador de valores faltantes**\n",
    "\n",
    "Se utiliza para que el modelo de machine learning pueda **aprender** que, en ciertos casos, los valores faltantes también pueden contener información útil. Esta técnica combina dos pasos:\n",
    "\n",
    "1. **Rellenar los valores faltantes con un número (como 0):** Esto permite que el modelo siga funcionando sin problemas matemáticos, porque todos los datos están completos.\n",
    "\n",
    "2. **Agregar una columna adicional (indicador de faltantes):** Esta columna será 1 si el valor original estaba faltante y 0 si no. De esta manera, el modelo sabe que, en esa fila específica, hubo un valor que originalmente estaba ausente.\n",
    "\n",
    "### ¿Por qué se utiliza esta técnica?\n",
    "\n",
    "1. **Captura información adicional**: A veces, el hecho de que un valor esté faltando puede ser relevante por sí mismo. Por ejemplo, si estás trabajando con datos médicos y un paciente no proporciona cierta información, eso podría ser indicativo de algo importante (como que no se hizo una prueba, o no quiso dar un dato específico). Al agregar la columna de indicador, le estás diciendo al modelo: \"Toma en cuenta que este dato estaba originalmente ausente\".\n",
    "\n",
    "2. **Mejora la precisión del modelo**: En lugar de simplemente eliminar los valores faltantes (lo que podría llevar a una pérdida de información valiosa) o rellenarlos con una media o mediana (lo que puede diluir los datos), el indicador de valores faltantes ayuda al modelo a captar patrones relacionados con la ausencia de datos, lo que puede mejorar su precisión.\n",
    "\n",
    "3. **Mantiene la mayor cantidad de datos posible**: Esta técnica permite usar todo el dataset sin tener que eliminar filas, lo cual es especialmente útil cuando el dataset no es muy grande o cuando eliminar datos podría llevar a un sesgo.\n",
    "\n",
    "**Ejemplo: Imputación de Valores Faltantes**\n",
    "\n",
    "Supongamos que algunos valores en el dataset están faltando. Vamos a generar valores faltantes intencionalmente para demostrar cómo manejarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>Category_A</th>\n",
       "      <th>Category_B</th>\n",
       "      <th>Category_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.001126</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680 -0.001126  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  Category_A  Category_B  Category_C  \n",
       "0 -0.002592  0.019907 -0.017646         1.0         0.0         0.0  \n",
       "1 -0.039493 -0.068332 -0.092204         0.0         0.0         1.0  \n",
       "2 -0.002592  0.002861 -0.025930         1.0         0.0         0.0  \n",
       "3  0.034309  0.022688 -0.009362         0.0         0.0         1.0  \n",
       "4 -0.002592 -0.031988 -0.046641         0.0         1.0         0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Introducir valores faltantes intencionalmente\n",
    "X.loc[::10, 'bmi'] = np.nan  # Introducir NaN en cada décima fila de la columna 'bmi'\n",
    "\n",
    "# Crear un imputador que reemplaza los valores faltantes con la media\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X['bmi'] = imputer.fit_transform(X[['bmi']])\n",
    "\n",
    "# Mostrar los datos transformados\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Estandarización y Normalización (Standardization and Normalization)**\n",
    "\n",
    "**Estandarización** y **normalización** son técnicas para escalar las características de los datos a una escala similar, lo que puede mejorar el rendimiento de muchos algoritmos de ML.\n",
    "\n",
    "**Justificación de la estandarización y normalización**\n",
    "\n",
    "La **estandarización** y la **normalización** son técnicas utilizadas para transformar los datos de manera que los modelos de machine learning puedan aprender mejor de ellos. Aunque parecen similares, tienen propósitos y aplicaciones ligeramente diferentes.\n",
    "\n",
    "### 1. ¿Por qué son necesarias?\n",
    "\n",
    "Los modelos de machine learning hacen muchos cálculos matemáticos usando los datos. Si las variables (o columnas) del dataset tienen escalas muy diferentes, pueden crear problemas:\n",
    "\n",
    "- **Impacto desproporcionado**: Variables con valores más grandes pueden dominar el proceso de entrenamiento del modelo. Por ejemplo, si tienes dos variables, \"edad\" (que varía entre 0 y 100) y \"ingresos\" (que puede variar entre 0 y 100,000), el modelo podría darle más importancia a \"ingresos\" solo porque sus números son más grandes, aunque ambas variables sean igualmente importantes.\n",
    "\n",
    "- **Mejora de la eficiencia del entrenamiento**: Muchos algoritmos de machine learning (como regresión logística, máquinas de soporte vectorial (SVM), redes neuronales, etc.) funcionan mejor cuando los datos están en una escala similar. Esto ayuda a que el modelo converja (aprenda) más rápidamente y de manera más estable.\n",
    "\n",
    "### 2. ¿Qué es la estandarización?\n",
    "\n",
    "**Estandarización** significa transformar los datos para que tengan una **media (promedio) de 0 y una desviación estándar de 1**. Esto se hace restando la media de cada valor y luego dividiendo por la desviación estándar. \n",
    "\n",
    "- **¿Cuándo utilizarla?**: \n",
    "  - Cuando los datos siguen (o aproximadamente siguen) una distribución normal (campana de Gauss).\n",
    "  - Es útil para algoritmos que asumen que los datos están distribuidos normalmente o que son sensibles a las escalas, como regresión lineal, SVM, y redes neuronales.\n",
    "\n",
    "### 3. ¿Qué es la normalización?\n",
    "\n",
    "**Normalización** significa escalar los datos para que caigan en un rango específico, generalmente de **0 a 1**. Esto se hace restando el valor mínimo de cada valor y luego dividiendo por el rango (valor máximo - valor mínimo).\n",
    "\n",
    "- **¿Cuándo utilizarla?**: \n",
    "  - Cuando los datos no siguen una distribución normal y pueden tener diferentes escalas.\n",
    "  - Es especialmente útil en algoritmos que miden distancias entre puntos, como K-Nearest Neighbors (KNN) o algoritmos de clustering (agrupamiento).\n",
    "\n",
    "### 4. Diferencias clave entre estandarización y normalización\n",
    "\n",
    "- **Estandarización** es útil cuando queremos que los datos tengan una media de 0 y una desviación estándar de 1, manteniendo la forma de la distribución original.\n",
    "- **Normalización** es útil cuando queremos limitar el rango de los datos a un intervalo específico, como 0 a 1, lo que puede ser necesario para algoritmos sensibles a la magnitud de los valores.\n",
    "\n",
    "### Resumen\n",
    "\n",
    "- **Estandarización**: Para datos con distribución normal o cuando el algoritmo asume una distribución normal.\n",
    "- **Normalización**: Para datos con diferentes escalas o cuando el algoritmo se basa en distancias.\n",
    "\n",
    "Ambas técnicas ayudan a que los modelos aprendan de manera más eficiente y precisa, evitando que ciertas variables dominen el entrenamiento solo por su escala o rango.\n",
    "\n",
    "**Ejemplo: Estandarización de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>Category_A</th>\n",
       "      <th>Category_B</th>\n",
       "      <th>Category_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800500</td>\n",
       "      <td>1.065488</td>\n",
       "      <td>4.811607e-18</td>\n",
       "      <td>0.459841</td>\n",
       "      <td>-0.929746</td>\n",
       "      <td>-0.732065</td>\n",
       "      <td>-0.912451</td>\n",
       "      <td>-0.054499</td>\n",
       "      <td>0.418531</td>\n",
       "      <td>-0.370989</td>\n",
       "      <td>1.423867</td>\n",
       "      <td>-0.670209</td>\n",
       "      <td>-0.749558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039567</td>\n",
       "      <td>-0.938537</td>\n",
       "      <td>-1.117196e+00</td>\n",
       "      <td>-0.553505</td>\n",
       "      <td>-0.177624</td>\n",
       "      <td>-0.402886</td>\n",
       "      <td>1.564414</td>\n",
       "      <td>-0.830301</td>\n",
       "      <td>-1.436589</td>\n",
       "      <td>-1.938479</td>\n",
       "      <td>-0.702313</td>\n",
       "      <td>-0.670209</td>\n",
       "      <td>1.334119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.793307</td>\n",
       "      <td>1.065488</td>\n",
       "      <td>1.011350e+00</td>\n",
       "      <td>-0.119214</td>\n",
       "      <td>-0.958674</td>\n",
       "      <td>-0.718897</td>\n",
       "      <td>-0.680245</td>\n",
       "      <td>-0.054499</td>\n",
       "      <td>0.060156</td>\n",
       "      <td>-0.545154</td>\n",
       "      <td>1.423867</td>\n",
       "      <td>-0.670209</td>\n",
       "      <td>-0.749558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.872441</td>\n",
       "      <td>-0.938537</td>\n",
       "      <td>-2.322948e-01</td>\n",
       "      <td>-0.770650</td>\n",
       "      <td>0.256292</td>\n",
       "      <td>0.525397</td>\n",
       "      <td>-0.757647</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.476983</td>\n",
       "      <td>-0.196823</td>\n",
       "      <td>-0.702313</td>\n",
       "      <td>-0.670209</td>\n",
       "      <td>1.334119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.113172</td>\n",
       "      <td>-0.938537</td>\n",
       "      <td>-7.823684e-01</td>\n",
       "      <td>0.459841</td>\n",
       "      <td>0.082726</td>\n",
       "      <td>0.327890</td>\n",
       "      <td>0.171178</td>\n",
       "      <td>-0.054499</td>\n",
       "      <td>-0.672502</td>\n",
       "      <td>-0.980568</td>\n",
       "      <td>-0.702313</td>\n",
       "      <td>1.492072</td>\n",
       "      <td>-0.749558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex           bmi        bp        s1        s2        s3  \\\n",
       "0  0.800500  1.065488  4.811607e-18  0.459841 -0.929746 -0.732065 -0.912451   \n",
       "1 -0.039567 -0.938537 -1.117196e+00 -0.553505 -0.177624 -0.402886  1.564414   \n",
       "2  1.793307  1.065488  1.011350e+00 -0.119214 -0.958674 -0.718897 -0.680245   \n",
       "3 -1.872441 -0.938537 -2.322948e-01 -0.770650  0.256292  0.525397 -0.757647   \n",
       "4  0.113172 -0.938537 -7.823684e-01  0.459841  0.082726  0.327890  0.171178   \n",
       "\n",
       "         s4        s5        s6  Category_A  Category_B  Category_C  \n",
       "0 -0.054499  0.418531 -0.370989    1.423867   -0.670209   -0.749558  \n",
       "1 -0.830301 -1.436589 -1.938479   -0.702313   -0.670209    1.334119  \n",
       "2 -0.054499  0.060156 -0.545154    1.423867   -0.670209   -0.749558  \n",
       "3  0.721302  0.476983 -0.196823   -0.702313   -0.670209    1.334119  \n",
       "4 -0.054499 -0.672502 -0.980568   -0.702313    1.492072   -0.749558  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Crear un escalador estándar\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Aplicar la estandarización a las características\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convertir el resultado a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Mostrar los datos estandarizados\n",
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo: Normalización Min-Max de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Aplicar la normalización a las características\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Convertir el resultado a DataFrame\u001b[39;00m\n\u001b[1;32m     10\u001b[0m X_normalized_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_normalized, columns\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Crear un escalador Min-Max\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Aplicar la normalización a las características\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convertir el resultado a DataFrame\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "\n",
    "# Mostrar los datos normalizados\n",
    "X_normalized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Transformación Logarítmica (Log Transformation)**\n",
    "\n",
    "La transformación logarítmica se utiliza para reducir la asimetría (skewness) de las características numéricas. Es útil cuando los datos tienen una distribución muy sesgada.\n",
    "\n",
    "**Justificación de transformación logaritmica**\n",
    "\n",
    "La **transformación logarítmica** es una técnica que se utiliza en machine learning y estadísticas para modificar la escala de los datos, especialmente cuando estos tienen una distribución sesgada o valores extremos (outliers).\n",
    "\n",
    "### ¿Por qué es necesaria la transformación logarítmica?\n",
    "\n",
    "1. **Reducir la asimetría (skewness) de los datos**: Si los datos tienen una distribución muy sesgada (por ejemplo, muchos valores pequeños y unos pocos valores muy grandes), puede ser difícil para el modelo de machine learning aprender patrones adecuados. La transformación logarítmica \"aplana\" esta distribución, haciendo que sea más simétrica y más fácil de manejar para los modelos.\n",
    "\n",
    "2. **Reducir el impacto de valores extremos**: Si hay algunos valores extremadamente altos (outliers) en los datos, estos pueden dominar el entrenamiento del modelo y llevar a predicciones menos precisas. La transformación logarítmica reduce el efecto de estos valores extremos, haciendo que el modelo sea más robusto y menos sensible a ellos.\n",
    "\n",
    "3. **Mejorar la relación entre características**: Algunas veces, las relaciones entre variables se vuelven más lineales después de aplicar una transformación logarítmica. Dado que muchos algoritmos de machine learning (como la regresión lineal) asumen que las relaciones entre las variables son lineales, aplicar una transformación logarítmica puede ayudar a mejorar el rendimiento del modelo.\n",
    "\n",
    "### ¿Cuándo utilizar la transformación logarítmica?\n",
    "\n",
    "La transformación logarítmica es útil en los siguientes casos:\n",
    "\n",
    "- **Cuando los datos están sesgados a la derecha**: Es decir, cuando hay muchos valores pequeños y pocos valores extremadamente grandes (como ingresos o precios).\n",
    "- **Cuando hay outliers positivos**: Valores muy altos que podrían distorsionar el entrenamiento del modelo.\n",
    "- **Cuando la relación entre las variables es multiplicativa o exponencial**: En estos casos, la transformación logarítmica puede convertir esta relación en una más lineal.\n",
    "\n",
    "### ¿Cómo funciona la transformación logarítmica?\n",
    "\n",
    "Aplicar una transformación logarítmica significa reemplazar cada valor \\(x\\) por su logaritmo \\(\\log(x)\\). \n",
    "\n",
    "- Para datos positivos, suele usarse el logaritmo natural (base e) o logaritmo base 10.\n",
    "- La transformación logarítmica comprime los rangos grandes y expande los rangos pequeños. Por ejemplo, pasa de valores como [1, 10, 100, 1000] a valores más manejables como [0, 1, 2, 3].\n",
    "\n",
    "**Ejemplo: Transformación Logarítmica de una Característica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         s1    s1_log\n",
      "0 -0.044223 -0.045231\n",
      "1 -0.008449 -0.008485\n",
      "2 -0.045599 -0.046672\n",
      "3  0.012191  0.012117\n",
      "4  0.003935  0.003927\n"
     ]
    }
   ],
   "source": [
    "# Aplicar una transformación logarítmica a la columna 's1'\n",
    "X['s1_log'] = np.log1p(X['s1'])  # np.log1p() aplica log(1 + x) para manejar ceros\n",
    "\n",
    "# Mostrar los datos transformados\n",
    "print(X[['s1', 's1_log']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Uso de `Pipeline` de Scikit-Learn para Transformación de Datos**\n",
    "\n",
    "El **`Pipeline`** de **Scikit-Learn** permite encadenar varias transformaciones de datos (como One-Hot Encoding, manejo de valores faltantes, escalado de características) y, finalmente, aplicar un estimador (modelo de ML). El `Pipeline` asegura que todas las transformaciones se apliquen de manera consistente a los datos de entrenamiento y de prueba.\n",
    "\n",
    "### **Ejemplo: Usando `Pipeline` para Encadenar Transformaciones de Datos**\n",
    "\n",
    "Vamos a utilizar el dataset de **Diabetes** de **Scikit-Learn** y aplicar múltiples técnicas de preprocesamiento utilizando `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Cargar el dataset de diabetes\n",
    "data = load_diabetes()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)  # Variables independientes\n",
    "y = pd.Series(data.target, name='Progression')  # Variable dependiente\n",
    "\n",
    "# Introducir una columna categórica simulada\n",
    "X['Category'] = np.random.choice(['A', 'B', 'C'], size=len(X))\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 2912.9207449826313\n",
      "Coefficient of Determination (R^2 Score): 0.4502005825336727\n"
     ]
    }
   ],
   "source": [
    "# Definir el preprocesamiento para características numéricas\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Manejo de valores faltantes\n",
    "    ('scaler', StandardScaler())  # Estandarización\n",
    "])\n",
    "\n",
    "# Definir el preprocesamiento para características categóricas\n",
    "categorical_features = ['Category']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-Hot Encoding\n",
    "])\n",
    "\n",
    "# Crear un transformador de columnas (ColumnTransformer) que aplica las transformaciones por tipo de dato\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Crear un Pipeline que aplica el preprocesador seguido de un modelo de regresión lineal\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Entrenar el Pipeline en el conjunto de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Coefficient of Determination (R^2 Score): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explicación del Código:**\n",
    "\n",
    "1. **División del Dataset**: Dividimos los datos en conjuntos de entrenamiento y prueba utilizando `train_test_split`.\n",
    "\n",
    "2. **Transformador de Características Numéricas**: Definimos un `Pipeline` llamado `numeric_transformer` que maneja los valores faltantes utilizando la media y estandariza las características numéricas.\n",
    "\n",
    "3. **Transformador de Características Categóricas**: Definimos un `Pipeline` llamado `categorical_transformer` que aplica **One-Hot Encoding** a las características categóricas.\n",
    "\n",
    "4. **ColumnTransformer**: Usamos `ColumnTransformer` para aplicar diferentes transformaciones a las características numéricas y categóricas. Esto nos permite transformar tanto datos numéricos como categóricos en un solo paso.\n",
    "\n",
    "5. **Pipeline Completo**: Creamos un `Pipeline` completo que aplica las transformaciones de preprocesamiento (definidas por `preprocessor`) y luego entrena un modelo de regresión lineal (`LinearRegression`).\n",
    "\n",
    "6. **Entrenamiento y Predicción**: Entrenamos el `Pipeline` con los datos de entrenamiento y luego realizamos predicciones en los datos de prueba.\n",
    "\n",
    "7. **Evaluación del Modelo**: Evaluamos el rendimiento del modelo utilizando el error cuadrático medio (MSE) y el coeficiente de determinación (R²).\n",
    "\n",
    "### **Beneficios de Usar `Pipeline` de Scikit-Learn:**\n",
    "\n",
    "- **Simplicidad y Limpieza del Código**: Permite aplicar varias transformaciones y estimadores en una cadena limpia y clara.\n",
    "- **Consistencia**: Asegura que todas las transformaciones se aplican de la misma manera a los datos de entrenamiento y de prueba.\n",
    "- **Repetibilidad**: Facilita la replicación del flujo de trabajo para nuevos datos.\n",
    "- **Facilidad de Ajuste de Hiperparámetros**: Facilita el uso de herramientas de búsqueda de hiperparámetros como `GridSearchCV`.\n",
    "\n",
    "### **Conclusión**\n",
    "\n",
    "El uso de `Pipeline` en Scikit-Learn es fundamental para construir flujos de trabajo de Machine Learning eficientes y reproducibles. Encadenar transformaciones de datos y modelos en un solo objeto asegura que todas las transformaciones se apliquen de manera consistente, mejorando así la calidad del modelo y simplificando el código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curso-ine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
